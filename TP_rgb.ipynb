{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HUiLzn60xua"
      },
      "source": [
        "# **Multiclass Neural Network for Tifinagh Character Recognition**\n",
        "\n",
        "This notebook implements a multilayer perceptron (MLP) to classify handwritten Tifinagh characters from the AMHCD dataset (28,182 images, 33 classes). It includes data preprocessing, model training, hyperparameter tuning, cross-validation, and evaluation, with bonus features: L2 regularization, Adam optimizer, K-fold cross-validation, and data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8olLt6R07Ig"
      },
      "source": [
        "## **1.Imports and Setup**\n",
        "Import required libraries and set random seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P1zZDz43JX4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d03b77-af88-48e4-8028-a4007fe037c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.11.0.86\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k61HG7FYXr5P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import zipfile\n",
        "import itertools\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm import trange, tqdm\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJmqWYzD1AyY"
      },
      "source": [
        "## **2.Activation Functions**\n",
        "Define ReLU and Softmax activations for the MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E28ZcOFF1Y1Q"
      },
      "outputs": [],
      "source": [
        "# Activation Functions\n",
        "def relu(x):\n",
        "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
        "    assert isinstance(x, np.ndarray), \"Input to ReLU must be a numpy array\"\n",
        "    result = np.maximum(0, x)\n",
        "    assert np.all(result >= 0), \"ReLU output must be non-negative\"\n",
        "    return result\n",
        "\n",
        "def relu_derivative(x):\n",
        "    \"\"\"Derivative of ReLU: 1 if x > 0, else 0\"\"\"\n",
        "    assert isinstance(x, np.ndarray), \"Input to ReLU derivative must be a numpy array\"\n",
        "    result = np.where(x > 0, 1, 0)\n",
        "    assert np.all((result == 0) | (result == 1)), \"ReLU derivative must be 0 or 1\"\n",
        "    return result\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Softmax activation: exp(x) / sum(exp(x))\"\"\"\n",
        "    assert isinstance(x, np.ndarray), \"Input to softmax must be a numpy array\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    result = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "    assert np.all((result >= 0) & (result <= 1)), \"Softmax output must be in [0,1]\"\n",
        "    assert np.allclose(np.sum(result, axis=1), 1), \"Softmax output must sum to 1\"\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHciEg601Nza"
      },
      "source": [
        "## **3.Data Augmentation and Preprocessing**\n",
        "Functions for augmenting images (rotation, translation) and preprocessing (grayscale, resize, normalize)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UCvnazfp1kUK"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation\n",
        "def augment_image(image, max_rotation=10, max_translation=3):\n",
        "    \"\"\"Apply random rotation and translation to a 32x32 grayscale image\"\"\"\n",
        "    assert image.shape == (32, 32), \"Image must be 32x32\"\n",
        "    angle = np.random.uniform(-max_rotation, max_rotation)\n",
        "    M = cv2.getRotationMatrix2D((16, 16), angle, 1.0)\n",
        "    rotated = cv2.warpAffine(image, M, (32, 32), borderMode=cv2.BORDER_REPLICATE)\n",
        "    tx = np.random.uniform(-max_translation, max_translation)\n",
        "    ty = np.random.uniform(-max_translation, max_translation)\n",
        "    M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "    translated = cv2.warpAffine(rotated, M, (32, 32), borderMode=cv2.BORDER_REPLICATE)\n",
        "    return translated.flatten()\n",
        "\n",
        "# Image Preprocessing\n",
        "def load_and_preprocess_image(image_path, data_dir, target_size=(32, 32)):\n",
        "    \"\"\"Load and preprocess image: grayscale, resize, normalize\"\"\"\n",
        "    full_path = os.path.join(data_dir, image_path)\n",
        "    assert os.path.exists(full_path), f\"Image not found: {full_path}\"\n",
        "    img = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
        "    assert img is not None, f\"Failed to load image: {full_path}\"\n",
        "    img = cv2.resize(img, target_size)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return img.flatten()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm339t3M1YSw"
      },
      "source": [
        "## **4.Visualization**\n",
        "Function to display sample images per Tifinagh class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nA2ZVf9E68cI"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "def display_sample_images_from_paths(labels_df, data_dir, path_column='image_path', label_column='label',\n",
        "                                     num_samples=33, samples_per_row=11, random_state=83):\n",
        "    \"\"\"Display a grid of sample grayscale images, one per class\"\"\"\n",
        "    assert path_column in labels_df.columns, f\"Missing column: {path_column}\"\n",
        "    assert label_column in labels_df.columns, f\"Missing column: {label_column}\"\n",
        "\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    unique_labels = labels_df[label_column].unique()\n",
        "    num_samples = min(num_samples, len(unique_labels))\n",
        "    selected_indices = [rng.choice(labels_df[labels_df[label_column] == lbl].index)\n",
        "                        for lbl in rng.choice(unique_labels, size=num_samples, replace=False)]\n",
        "    selected_df = labels_df.loc[selected_indices].sort_values(by=label_column)\n",
        "\n",
        "    num_rows = (num_samples + samples_per_row - 1) // samples_per_row\n",
        "    fig, axes = plt.subplots(num_rows, samples_per_row, figsize=(samples_per_row * 2, num_rows * 3))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i, (path, label) in enumerate(zip(selected_df[path_column], selected_df[label_column])):\n",
        "        try:\n",
        "            img = Image.open(os.path.join(data_dir, path))\n",
        "            axes[i].imshow(img, cmap='gray')\n",
        "            axes[i].set_title(f\"Label: {label}\", fontsize=17)\n",
        "            axes[i].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {path}: {e}\")\n",
        "            axes[i].set_title(\"Error\")\n",
        "            axes[i].axis('off')\n",
        "\n",
        "    for ax in axes[num_samples:]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOvgoYDC1o07"
      },
      "source": [
        "## **5.Neural Network Implementation**\n",
        "MLP class with forward propagation, backpropagation, and training logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GztogE3-6tZ8"
      },
      "outputs": [],
      "source": [
        "# Neural Network Class\n",
        "class MulticlassNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01, l2_lambda=0.01, optimizer='sgd', beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        \"\"\"Initialize MLP with specified architecture and parameters\"\"\"\n",
        "        assert len(layer_sizes) >= 2, \"At least 2 layers required\"\n",
        "        assert all(size > 0 for size in layer_sizes), \"Layer sizes must be positive\"\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.optimizer = optimizer.lower()\n",
        "        self.beta1, self.beta2, self.epsilon = beta1, beta2, epsilon\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.m_weights, self.v_weights = [], []\n",
        "        self.m_biases, self.v_biases = [], []\n",
        "        self.t = 0\n",
        "\n",
        "        # He initialization\n",
        "        np.random.seed(42)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i])\n",
        "            b = np.zeros((1, layer_sizes[i + 1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "            self.m_weights.append(np.zeros_like(w))\n",
        "            self.v_weights.append(np.zeros_like(w))\n",
        "            self.m_biases.append(np.zeros_like(b))\n",
        "            self.v_biases.append(np.zeros_like(b))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward propagation\"\"\"\n",
        "        self.z_values, self.activations = [], [X]\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            Z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(Z)\n",
        "            self.activations.append(relu(Z))\n",
        "        Z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(Z)\n",
        "        output = softmax(Z)\n",
        "        self.activations.append(output)\n",
        "        return output\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Cross-entropy loss with L2 regularization\"\"\"\n",
        "        m = y_true.shape[0]\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        cross_entropy = -np.sum(y_true * np.log(y_pred_clipped)) / m\n",
        "        l2_term = (self.l2_lambda / (2 * m)) * sum(np.sum(np.square(W)) for W in self.weights)\n",
        "        total_loss = cross_entropy + l2_term\n",
        "        assert not np.isnan(total_loss), \"Loss is NaN\"\n",
        "        return total_loss\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        \"\"\"Compute accuracy\"\"\"\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        true_labels = np.argmax(y_true, axis=1)\n",
        "        return np.mean(predictions == true_labels)\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"Backpropagation\"\"\"\n",
        "        m = X.shape[0]\n",
        "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
        "        dZ = self.activations[-1] - y\n",
        "        self.d_weights[-1] = (np.dot(self.activations[-2].T, dZ) + self.l2_lambda * self.weights[-1]) / m\n",
        "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        dA_prev = dZ\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            dA = np.dot(dA_prev, self.weights[i + 1].T)\n",
        "            dZ = dA * relu_derivative(self.z_values[i])\n",
        "            self.d_weights[i] = (np.dot(self.activations[i].T, dZ) + self.l2_lambda * self.weights[i]) / m\n",
        "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "            dA_prev = dZ\n",
        "\n",
        "    def update_parameters(self):\n",
        "        \"\"\"Update parameters using SGD or Adam\"\"\"\n",
        "        if self.optimizer == 'sgd':\n",
        "            for i in range(len(self.weights)):\n",
        "                self.weights[i] -= self.learning_rate * self.d_weights[i]\n",
        "                self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "        elif self.optimizer == 'adam':\n",
        "            self.t += 1\n",
        "            for i in range(len(self.weights)):\n",
        "                self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * self.d_weights[i]\n",
        "                self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * (self.d_weights[i] ** 2)\n",
        "                self.m_biases[i] = self.beta1 * self.m_biases[i] + (1 - self.beta1) * self.d_biases[i]\n",
        "                self.v_biases[i] = self.beta2 * self.v_biases[i] + (1 - self.beta2) * (self.d_biases[i] ** 2)\n",
        "                m_w_hat = self.m_weights[i] / (1 - self.beta1 ** self.t)\n",
        "                v_w_hat = self.v_weights[i] / (1 - self.beta2 ** self.t)\n",
        "                m_b_hat = self.m_biases[i] / (1 - self.beta1 ** self.t)\n",
        "                v_b_hat = self.v_biases[i] / (1 - self.beta2 ** self.t)\n",
        "                self.weights[i] -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
        "                self.biases[i] -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, epochs=100, batch_size=100, augment=False, early_stopping=True, verbose=1):\n",
        "        \"\"\"Train the MLP\"\"\"\n",
        "        train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "        best_val_loss = np.inf\n",
        "        best_weights = [w.copy() for w in self.weights]\n",
        "        best_biases = [b.copy() for b in self.biases]\n",
        "        patience, patience_counter = 10, 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled, y_shuffled = X[indices], y[indices]\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i + batch_size]\n",
        "                y_batch = y_shuffled[i:i + batch_size]\n",
        "                if augment:\n",
        "                    X_batch = np.array([augment_image(img.reshape(32, 32)) for img in X_batch])\n",
        "                outputs = self.forward(X_batch)\n",
        "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
        "                self.backward(X_batch, y_batch)\n",
        "                self.update_parameters()\n",
        "\n",
        "            train_pred = self.forward(X)\n",
        "            val_pred = self.forward(X_val)\n",
        "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
        "            val_loss = self.compute_loss(y_val, val_pred)\n",
        "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
        "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "            end_time = time.time()\n",
        "            epoch_time = end_time - start_time\n",
        "\n",
        "            if early_stopping:\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_weights = [w.copy() for w in self.weights]\n",
        "                    best_biases = [b.copy() for b in self.biases]\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        if verbose >= 1:\n",
        "                            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                        self.weights = best_weights\n",
        "                        self.biases = best_biases\n",
        "                        break\n",
        "\n",
        "            log = (epoch % 10 == 0 or epoch == epochs - 1)\n",
        "            if verbose == 2 and log:\n",
        "                print(f\"Epoch {epoch + 1:03d} | Time: {epoch_time:.2f}s | \"\n",
        "                      f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "                      f\"Train Acc: {train_accuracy:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
        "            elif verbose == 1 and log:\n",
        "                print(f\"\\nEpoch {epoch + 1:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "                      f\"Train Acc: {train_accuracy:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels\"\"\"\n",
        "        outputs = self.forward(X)\n",
        "        return np.argmax(outputs, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4PaxqZQ1y29"
      },
      "source": [
        "## **6.K-Fold Cross-Validation**\n",
        "Function for K-fold cross-validation to evaluate model robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4S8q1bMm6waJ"
      },
      "outputs": [],
      "source": [
        "# K-Fold Cross-Validation\n",
        "def k_fold_cross_validation(X, y_one_hot, layer_sizes, k=5, epochs=100, batch_size=32,\n",
        "                            learning_rate=0.001, l2_lambda=0.01, optimizer='sgd',\n",
        "                            augment=False, early_stopping=False, verbose=1):\n",
        "    assert isinstance(X, np.ndarray) and isinstance(y_one_hot, np.ndarray), \"X and y_one_hot must be numpy arrays\"\n",
        "    assert X.shape[0] == y_one_hot.shape[0], \"X and y_one_hot must have the same number of samples\"\n",
        "    assert isinstance(layer_sizes, list) and len(layer_sizes) >= 2, \"layer_sizes must be a list with at least 2 elements\"\n",
        "    assert isinstance(k, int) and k > 1, \"k must be an integer greater than 1\"\n",
        "    assert isinstance(epochs, int) and epochs > 0, \"Epochs must be a positive integer\"\n",
        "    assert isinstance(batch_size, int) and batch_size > 0, \"Batch size must be a positive integer\"\n",
        "    assert optimizer.lower() in ['sgd', 'adam'], \"Optimizer must be 'sgd' or 'adam'\"\n",
        "\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    fold_train_losses, fold_val_losses = [], []\n",
        "    fold_train_accuracies, fold_val_accuracies = [], []\n",
        "\n",
        "    all_splits = list(kf.split(X))\n",
        "\n",
        "    for fold in trange(k, desc=\"Cross-validation folds\", disable=(verbose == 0)):\n",
        "        train_idx, val_idx = all_splits[fold]\n",
        "\n",
        "        if verbose >= 1:\n",
        "            print(f\"\\n=========================== Fold {fold + 1}/{k} ===========================================\\n\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train_one_hot, y_val_one_hot = y_one_hot[train_idx], y_one_hot[val_idx]\n",
        "\n",
        "        nn = MulticlassNeuralNetwork(\n",
        "            layer_sizes, learning_rate=learning_rate, l2_lambda=l2_lambda, optimizer=optimizer\n",
        "        )\n",
        "\n",
        "        train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
        "            X_train, y_train_one_hot, X_val, y_val_one_hot,\n",
        "            epochs=epochs, batch_size=batch_size,\n",
        "            augment=augment, early_stopping=early_stopping, verbose=verbose\n",
        "        )\n",
        "\n",
        "        fold_train_losses.append(train_losses)\n",
        "        fold_val_losses.append(val_losses)\n",
        "        fold_train_accuracies.append(train_accuracies)\n",
        "        fold_val_accuracies.append(val_accuracies)\n",
        "\n",
        "        final_val_accuracy = val_accuracies[-1]\n",
        "        if verbose >= 2:\n",
        "            y_val_pred = nn.predict(X_val)\n",
        "            print(f\"\\n===========================Fold {fold + 1} Classification Report: =======================\")\n",
        "            print(classification_report(np.argmax(y_val_one_hot, axis=1), y_val_pred))\n",
        "\n",
        "        if verbose >= 1:\n",
        "            print(f\"Fold {fold + 1} completed in {time.time() - start_time:.2f} seconds\")\n",
        "            print(f\"Final validation accuracy: {final_val_accuracy:.4f}\")\n",
        "\n",
        "    avg_train_losses = np.mean(fold_train_losses, axis=0)\n",
        "    avg_val_losses = np.mean(fold_val_losses, axis=0)\n",
        "    avg_train_accuracies = np.mean(fold_train_accuracies, axis=0)\n",
        "    avg_val_accuracies = np.mean(fold_val_accuracies, axis=0)\n",
        "\n",
        "    if verbose >= 1:\n",
        "        print(f\"\\n=========================== Cross-Validation Summary ===========================\")\n",
        "        print(f\"Average final validation accuracy: {avg_val_accuracies[-1]:.4f}\")\n",
        "        print(f\"Metrics computed over {epochs} epochs\")\n",
        "\n",
        "    return avg_train_losses, avg_val_losses, avg_train_accuracies, avg_val_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEErF4d116EV"
      },
      "source": [
        "## **7.Dataset Loading and Preprocessing**\n",
        "Load AMHCD dataset, preprocess images, encode labels, and split data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoZNo7cZSmy3"
      },
      "source": [
        "### **7.1. Extracting & Loading AMHCD DataSet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tWzCzSLhpTx2"
      },
      "outputs": [],
      "source": [
        "def cleanup(base_dir=\"/content/\", temp_dir=\"temp\"):\n",
        "    temp_dir = os.path.join(base_dir, temp_dir)\n",
        "    shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "    print(f\"{temp_dir} Cleaned up.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drGBaLikpVtA",
        "outputId": "dae93fda-019f-4b52-d660-87211d8eda3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/amhcd Cleaned up.\n",
            "Path to dataset files: /content/amhcd\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"benaddym/amazigh-handwritten-character-database-amhcd\")\n",
        "\n",
        "target_dir = \"/content/amhcd\"\n",
        "cleanup(temp_dir=target_dir)\n",
        "shutil.copytree(path, target_dir, dirs_exist_ok=True)\n",
        "\n",
        "print(\"Path to dataset files:\", target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LPtjpCrD_fW2"
      },
      "outputs": [],
      "source": [
        "latin_labels_txt    = target_dir + '/labels/labels/33-common-latin-tifinagh.txt'\n",
        "\n",
        "tifinagh_labels_txt = target_dir + '/labels/labels/sorted-33-common-tifinagh.txt'\n",
        "images_root         = target_dir + '/AMHCD_64/AMHCD_64'\n",
        "output_csv          = target_dir + '/rgb-labels-map.csv'\n",
        "\n",
        "# Lire les labels Latin et les glyphes Tifinagh\n",
        "with open(latin_labels_txt, 'r', encoding='utf-8') as f:\n",
        "    ascii_labels = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "with open(tifinagh_labels_txt, 'r', encoding='utf-8') as f:\n",
        "    tif_chars = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "#  Vérification\n",
        "if len(ascii_labels) != len(tif_chars):\n",
        "    print(f\"Erreur : {len(ascii_labels)} labels Latin ≠ {len(tif_chars)} glyphes Tifinagh\")\n",
        "    sys.exit(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3Y83mzUFTha",
        "outputId": "f9e66474-da88-4678-c312-e224c3ffb10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Généré /content/amhcd/rgb-labels-map.csv avec 25740 entrées.\n"
          ]
        }
      ],
      "source": [
        "# Génération du CSV\n",
        "with open(output_csv, 'w', encoding='utf-8', newline='') as out:\n",
        "    for latin, tif in zip(ascii_labels, tif_chars):\n",
        "        folder = os.path.join(images_root, latin)\n",
        "        if not os.path.isdir(folder):\n",
        "            print(f\"Dossier introuvable, je passe : {folder}\")\n",
        "            continue\n",
        "        for fname in sorted(os.listdir(folder)):\n",
        "            # filtrer si nécessaire par extension d’image\n",
        "            if not fname.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tif','.tiff')):\n",
        "                continue\n",
        "            rel_path = f\"{images_root}/{latin}/{fname}\"\n",
        "            # Écrire UNE ligne contenant chemin + glyphe\n",
        "            out.write(f\"{rel_path},{tif}\\n\")\n",
        "\n",
        "print(f\"Généré {output_csv} avec {sum(len(os.listdir(os.path.join(images_root,l))) for l in ascii_labels if os.path.isdir(os.path.join(images_root,l)))} entrées.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5DiSUotZG7b8"
      },
      "outputs": [],
      "source": [
        "labels_df = pd.read_csv(output_csv, names=['image_path', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCUQE90FUVDA",
        "outputId": "d4d3602f-4771-4c22-c229-ed02221104e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample image paths:\n",
            "/content/amhcd/AMHCD_64/AMHCD_64/ya/ya_1.jpeg -> Exists: True\n",
            "/content/amhcd/AMHCD_64/AMHCD_64/ya/ya_10.jpeg -> Exists: True\n",
            "/content/amhcd/AMHCD_64/AMHCD_64/ya/ya_100.jpeg -> Exists: True\n",
            "/content/amhcd/AMHCD_64/AMHCD_64/ya/ya_101.jpeg -> Exists: True\n",
            "/content/amhcd/AMHCD_64/AMHCD_64/ya/ya_102.jpeg -> Exists: True\n",
            "\n",
            "Loaded 25740 samples with 33 unique classes .\n"
          ]
        }
      ],
      "source": [
        "# Debugging: Print sample paths\n",
        "print(\"Sample image paths:\")\n",
        "for path in labels_df['image_path'].head(5):\n",
        "    full_path = os.path.join(target_dir, path)\n",
        "    print(f\"{full_path} -> Exists: {os.path.exists(full_path)}\")\n",
        "\n",
        "print(f\"\\nLoaded {len(labels_df)} samples with {labels_df['label'].nunique ()} unique classes .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSICsY_lUqUW",
        "outputId": "b591eaaa-677e-4b14-ea6b-a87a5fb600bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution summary:\n",
            "Min: 780, Max: 780, Mean: 780.00\n"
          ]
        }
      ],
      "source": [
        "class_counts = labels_df['label'].value_counts()\n",
        "print(\"Class distribution summary:\")\n",
        "print(f\"Min: {class_counts.min()}, Max: {class_counts.max()}, Mean: {class_counts.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUMCCi6rUglE",
        "outputId": "c3f220ea-6148-476b-ffb6-e76e7e2b4781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label encoding ...\n",
            "==================== Data with label encoded: ======================\n",
            "                                            image_path label  label_encoded\n",
            "0        /content/amhcd/AMHCD_64/AMHCD_64/ya/ya_1.jpeg     ⴰ              0\n",
            "1       /content/amhcd/AMHCD_64/AMHCD_64/ya/ya_10.jpeg     ⴰ              0\n",
            "2      /content/amhcd/AMHCD_64/AMHCD_64/ya/ya_100.jpeg     ⴰ              0\n",
            "3      /content/amhcd/AMHCD_64/AMHCD_64/ya/ya_101.jpeg     ⴰ              0\n",
            "4      /content/amhcd/AMHCD_64/AMHCD_64/ya/ya_102.jpeg     ⴰ              0\n",
            "...                                                ...   ...            ...\n",
            "25735   /content/amhcd/AMHCD_64/AMHCD_64/yu/yu_95.jpeg     ⵓ             20\n",
            "25736   /content/amhcd/AMHCD_64/AMHCD_64/yu/yu_96.jpeg     ⵓ             20\n",
            "25737   /content/amhcd/AMHCD_64/AMHCD_64/yu/yu_97.jpeg     ⵓ             20\n",
            "25738   /content/amhcd/AMHCD_64/AMHCD_64/yu/yu_98.jpeg     ⵓ             20\n",
            "25739   /content/amhcd/AMHCD_64/AMHCD_64/yu/yu_99.jpeg     ⵓ             20\n",
            "\n",
            "[25740 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Label encoding ...\")\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "print(\"==================== Data with label encoded: ======================\")\n",
        "print(labels_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zdbZdosHDQYP",
        "outputId": "c4231a3f-06df-48ce-c2f2-a4a6b4345368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "label  label_encoded\n",
              "ⴰ      0                780\n",
              "ⴱ      1                780\n",
              "ⴳ      2                780\n",
              "ⴳⵯ     3                780\n",
              "ⴷ      4                780\n",
              "ⴹ      5                780\n",
              "ⴻ      6                780\n",
              "ⴼ      7                780\n",
              "ⴽ      8                780\n",
              "ⴽⵯ     9                780\n",
              "ⵀ      10               780\n",
              "ⵃ      11               780\n",
              "ⵄ      12               780\n",
              "ⵅ      13               780\n",
              "ⵇ      14               780\n",
              "ⵉ      15               780\n",
              "ⵊ      16               780\n",
              "ⵍ      17               780\n",
              "ⵎ      18               780\n",
              "ⵏ      19               780\n",
              "ⵓ      20               780\n",
              "ⵔ      21               780\n",
              "ⵕ      22               780\n",
              "ⵖ      23               780\n",
              "ⵙ      24               780\n",
              "ⵚ      25               780\n",
              "ⵛ      26               780\n",
              "ⵜ      27               780\n",
              "ⵟ      28               780\n",
              "ⵡ      29               780\n",
              "ⵢ      30               780\n",
              "ⵣ      31               780\n",
              "ⵥ      32               780\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th>label_encoded</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ⴰ</th>\n",
              "      <th>0</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴱ</th>\n",
              "      <th>1</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴳ</th>\n",
              "      <th>2</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴳⵯ</th>\n",
              "      <th>3</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴷ</th>\n",
              "      <th>4</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴹ</th>\n",
              "      <th>5</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴻ</th>\n",
              "      <th>6</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴼ</th>\n",
              "      <th>7</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴽ</th>\n",
              "      <th>8</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⴽⵯ</th>\n",
              "      <th>9</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵀ</th>\n",
              "      <th>10</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵃ</th>\n",
              "      <th>11</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵄ</th>\n",
              "      <th>12</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵅ</th>\n",
              "      <th>13</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵇ</th>\n",
              "      <th>14</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵉ</th>\n",
              "      <th>15</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵊ</th>\n",
              "      <th>16</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵍ</th>\n",
              "      <th>17</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵎ</th>\n",
              "      <th>18</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵏ</th>\n",
              "      <th>19</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵓ</th>\n",
              "      <th>20</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵔ</th>\n",
              "      <th>21</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵕ</th>\n",
              "      <th>22</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵖ</th>\n",
              "      <th>23</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵙ</th>\n",
              "      <th>24</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵚ</th>\n",
              "      <th>25</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵛ</th>\n",
              "      <th>26</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵜ</th>\n",
              "      <th>27</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵟ</th>\n",
              "      <th>28</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵡ</th>\n",
              "      <th>29</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵢ</th>\n",
              "      <th>30</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵣ</th>\n",
              "      <th>31</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ⵥ</th>\n",
              "      <th>32</th>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(f\"Class distribution:\")\n",
        "# display(labels_df['label'].value_counts())\n",
        "display(labels_df[['label', 'label_encoded']].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GoyUb1q_TC8N"
      },
      "outputs": [],
      "source": [
        "# Load images\n",
        "X = np.array([load_and_preprocess_image(path, target_dir) for path in labels_df['image_path']])\n",
        "y = labels_df['label_encoded'].values\n",
        "\n",
        "# Verify dimensions\n",
        "assert X.shape[0] == y.shape[0], \"Mismatch between number of images and labels\"\n",
        "assert X.shape[1] == 32 * 32, f\"Expected flattened image size of {32 * 32}, got {X.shape[1]}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6nwS6JecVIX6",
        "outputId": "c49167da-85de-4bce-f2f9-7691925be9e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_09c4a465-7934-4f88-bcb6-eab531702170\", \"sample_images1.png\", 904841)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = display_sample_images_from_paths(labels_df, target_dir, random_state=4)\n",
        "plt.savefig('sample_images1.png')\n",
        "files.download('sample_images1.png')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkNBfV2jVPts",
        "outputId": "4f28ab4c-c87b-4325-c65f-d2148df885bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 15444, Val: 5148, Test: 5148\n"
          ]
        }
      ],
      "source": [
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_one_hot = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_val_one_hot = one_hot_encoder.transform(y_val.reshape(-1, 1))\n",
        "y_test_one_hot = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
        "print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQTReXuzVo-8"
      },
      "source": [
        "# **8.Hyperparameter Tuning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Im1aUXMvGW-f"
      },
      "outputs": [],
      "source": [
        "layer_sizes = [32 * 32, 64, 32, num_classes]\n",
        "param_distributions = {\n",
        "    'learning_rate': [0.01, 0.001],\n",
        "    'l2_lambda': [0.01, 0.001],\n",
        "    'batch_size': [64, 32],\n",
        "    'optimizer': ['adam', 'sgd']\n",
        "}\n",
        "\n",
        "n_iter = 16\n",
        "\n",
        "# All possible combinations\n",
        "keys = list(param_distributions.keys())\n",
        "all_combinations = [\n",
        "    dict(zip(keys, values))\n",
        "    for values in itertools.product(*param_distributions.values())\n",
        "]\n",
        "\n",
        "# Shuffle and pick n_iter unique combinations\n",
        "random.shuffle(all_combinations)\n",
        "random_param_combinations  = all_combinations[:n_iter]\n",
        "\n",
        "assert len(random_param_combinations) == len(set(tuple(sorted(p.items())) for p in random_param_combinations)), \"Duplicate combinations found!\"\n",
        "\n",
        "results = []\n",
        "best_val_accuracy = 0\n",
        "best_params = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfvBHqp_WUXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e702a3-c61a-44b3-f2a8-97f5af2195f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRandom Search:   0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial 1/16: {'learning_rate': 0.01, 'l2_lambda': 0.001, 'batch_size': 64, 'optimizer': 'adam'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Cross-validation folds:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========================== Fold 1/5 ===========================================\n",
            "\n",
            "Epoch 001 | Time: 4.63s | Train Loss: 3.6134 | Val Loss: 3.5007 | Train Acc: 0.0322 | Val Acc: 0.0227\n",
            "Epoch 011 | Time: 4.50s | Train Loss: 3.5172 | Val Loss: 3.5002 | Train Acc: 0.0317 | Val Acc: 0.0246\n",
            "Epoch 021 | Time: 4.70s | Train Loss: 3.5172 | Val Loss: 3.4996 | Train Acc: 0.0317 | Val Acc: 0.0246\n",
            "Epoch 031 | Time: 4.80s | Train Loss: 3.5171 | Val Loss: 3.4990 | Train Acc: 0.0322 | Val Acc: 0.0227\n",
            "Epoch 041 | Time: 4.90s | Train Loss: 3.5184 | Val Loss: 3.4989 | Train Acc: 0.0314 | Val Acc: 0.0259\n",
            "Epoch 051 | Time: 5.20s | Train Loss: 3.5172 | Val Loss: 3.4991 | Train Acc: 0.0317 | Val Acc: 0.0246\n",
            "Epoch 061 | Time: 5.29s | Train Loss: 3.5168 | Val Loss: 3.5001 | Train Acc: 0.0322 | Val Acc: 0.0227\n",
            "Epoch 071 | Time: 5.10s | Train Loss: 3.5172 | Val Loss: 3.4987 | Train Acc: 0.0307 | Val Acc: 0.0288\n",
            "Epoch 081 | Time: 5.70s | Train Loss: 3.5172 | Val Loss: 3.4991 | Train Acc: 0.0313 | Val Acc: 0.0262\n",
            "Epoch 091 | Time: 11.60s | Train Loss: 3.5172 | Val Loss: 3.4996 | Train Acc: 0.0312 | Val Acc: 0.0269\n",
            "Epoch 100 | Time: 14.70s | Train Loss: 3.5171 | Val Loss: 3.4995 | Train Acc: 0.0322 | Val Acc: 0.0227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\n",
            "Cross-validation folds:  20%|██        | 1/5 [10:11<40:44, 611.16s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===========================Fold 1 Classification Report: =======================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        90\n",
            "           1       0.00      0.00      0.00        93\n",
            "           2       0.00      0.00      0.00        93\n",
            "           3       0.00      0.00      0.00       101\n",
            "           4       0.00      0.00      0.00        85\n",
            "           5       0.00      0.00      0.00        80\n",
            "           6       0.00      0.00      0.00       102\n",
            "           7       0.00      0.00      0.00        83\n",
            "           8       0.00      0.00      0.00        83\n",
            "           9       0.00      0.00      0.00       109\n",
            "          10       0.00      0.00      0.00        76\n",
            "          11       0.00      0.00      0.00        95\n",
            "          12       0.00      0.00      0.00        95\n",
            "          13       0.00      0.00      0.00        93\n",
            "          14       0.00      0.00      0.00        89\n",
            "          15       0.00      0.00      0.00        90\n",
            "          16       0.00      0.00      0.00        96\n",
            "          17       0.00      0.00      0.00        95\n",
            "          18       0.00      0.00      0.00        93\n",
            "          19       0.00      0.00      0.00        99\n",
            "          20       0.00      0.00      0.00       104\n",
            "          21       0.00      0.00      0.00        98\n",
            "          22       0.00      0.00      0.00       105\n",
            "          23       0.00      0.00      0.00       100\n",
            "          24       0.00      0.00      0.00        99\n",
            "          25       0.00      0.00      0.00       103\n",
            "          26       0.00      0.00      0.00        81\n",
            "          27       0.00      0.00      0.00        98\n",
            "          28       0.00      0.00      0.00       105\n",
            "          29       0.00      0.00      0.00        88\n",
            "          30       0.00      0.00      0.00       103\n",
            "          31       0.00      0.00      0.00        95\n",
            "          32       0.02      1.00      0.04        70\n",
            "\n",
            "    accuracy                           0.02      3089\n",
            "   macro avg       0.00      0.03      0.00      3089\n",
            "weighted avg       0.00      0.02      0.00      3089\n",
            "\n",
            "Fold 1 completed in 611.16 seconds\n",
            "Final validation accuracy: 0.0227\n",
            "\n",
            "=========================== Fold 2/5 ===========================================\n",
            "\n",
            "Epoch 001 | Time: 4.78s | Train Loss: 3.7098 | Val Loss: 3.5003 | Train Acc: 0.0304 | Val Acc: 0.0301\n",
            "Epoch 011 | Time: 5.09s | Train Loss: 3.5183 | Val Loss: 3.4995 | Train Acc: 0.0308 | Val Acc: 0.0282\n",
            "Epoch 021 | Time: 4.90s | Train Loss: 3.5174 | Val Loss: 3.5002 | Train Acc: 0.0314 | Val Acc: 0.0259\n",
            "Epoch 031 | Time: 5.00s | Train Loss: 3.5184 | Val Loss: 3.4997 | Train Acc: 0.0310 | Val Acc: 0.0275\n",
            "Epoch 041 | Time: 5.00s | Train Loss: 3.5167 | Val Loss: 3.5008 | Train Acc: 0.0314 | Val Acc: 0.0259\n",
            "Epoch 051 | Time: 5.00s | Train Loss: 3.5166 | Val Loss: 3.5009 | Train Acc: 0.0310 | Val Acc: 0.0275\n",
            "Epoch 061 | Time: 5.11s | Train Loss: 3.5167 | Val Loss: 3.4996 | Train Acc: 0.0318 | Val Acc: 0.0243\n",
            "Epoch 071 | Time: 5.20s | Train Loss: 3.5173 | Val Loss: 3.4980 | Train Acc: 0.0318 | Val Acc: 0.0243\n",
            "Epoch 081 | Time: 5.20s | Train Loss: 3.5169 | Val Loss: 3.4985 | Train Acc: 0.0306 | Val Acc: 0.0291\n"
          ]
        }
      ],
      "source": [
        "# ===================== Random Search + CV Loop =======================\n",
        "\n",
        "for i in trange(n_iter, desc=\"Random Search\"):\n",
        "    params = random_param_combinations[i]\n",
        "    print(f\"\\nTrial {i + 1}/{n_iter}: {params}\")\n",
        "    avg_train_losses, avg_val_losses, avg_train_accuracies, avg_val_accuracies = k_fold_cross_validation(\n",
        "        X_train, y_train_one_hot, layer_sizes, k=5, epochs=100, **params, augment=True, verbose=2\n",
        "    )\n",
        "    final_val_accuracy = avg_val_accuracies[-1]\n",
        "    results.append({'trial': i + 1, 'params': params, 'final_val_accuracy': final_val_accuracy})\n",
        "    if final_val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = final_val_accuracy\n",
        "        best_params = params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw0rwDnCpKZt"
      },
      "outputs": [],
      "source": [
        "# ===================== Result Summary & Best Pick ========================\n",
        "df = pd.DataFrame([\n",
        "    {\n",
        "        'Trial': r['trial'],\n",
        "        'learning_rate': r['params']['learning_rate'],\n",
        "        'l2_lambda': r['params']['l2_lambda'],\n",
        "        'batch_size': r['params']['batch_size'],\n",
        "        'optimizer': r['params']['optimizer'],\n",
        "        'val_accuracy': r['final_val_accuracy']\n",
        "    }\n",
        "    for r in results\n",
        "]).sort_values(by='val_accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "display(df)\n",
        "\n",
        "df.to_csv('rs_results_summary.csv', index=False)\n",
        "files.download('rs_results_summary.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB8xgDA6NT7H"
      },
      "outputs": [],
      "source": [
        "# # ===================== Result Summary & Best Pick ========================\n",
        "# results_df = pd.DataFrame([\n",
        "#     {\n",
        "#         'Trial': r['trial'],\n",
        "#         'learning_rate': r['params']['learning_rate'],\n",
        "#         'l2_lambda': r['params']['l2_lambda'],\n",
        "#         'batch_size': r['params']['batch_size'],\n",
        "#         'optimizer': r['params']['optimizer'],\n",
        "#         'val_accuracy': r['final_val_accuracy']\n",
        "#     }\n",
        "#     for r in results\n",
        "# ]).sort_values(by='val_accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# display(results_df)\n",
        "\n",
        "# results_df.to_csv('rs_results_summary.csv', index=False)\n",
        "# files.download('rs_results_summary.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvVIAm0F_Ikg"
      },
      "outputs": [],
      "source": [
        "# ========== Save best parameters and best validation accuracy + layer_sizes ==========\n",
        "with open(\"random_search_results.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"best_params\": best_params,\n",
        "        \"best_val_accuracy\": best_val_accuracy,\n",
        "        \"layer_sizes\": layer_sizes\n",
        "    }, f)\n",
        "\n",
        "files.download(\"random_search_results.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7aCNiSzbIte"
      },
      "source": [
        "## **Perform k-fold cross-validation with SGD & Adam**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUS9lshLz9rx"
      },
      "outputs": [],
      "source": [
        "# =================== Load saved random_search_results.json file ================\n",
        "with open(\"random_search_results.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    best_params = data[\"best_params\"]\n",
        "    layer_sizes = data[\"layer_sizes\"]\n",
        "    best_val_accuracy = data[\"best_val_accuracy\"]\n",
        "\n",
        "learning_rate = best_params['learning_rate']\n",
        "l2_lambda = best_params['l2_lambda']\n",
        "batch_size = best_params['batch_size']\n",
        "optimizer = best_params['optimizer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJdGcyCzcKNE"
      },
      "source": [
        "### **9.1.Perform k-fold cross-validation with SGD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKS9X6amwiqr"
      },
      "outputs": [],
      "source": [
        "# Perform k-fold cross-validation with SGD\n",
        "print(\"\\n========================= K-Fold Cross-Validation with SGD (Augmented): ===========================\")\n",
        "avg_train_losses_sgd, avg_val_losses_sgd, avg_train_accuracies_sgd, avg_val_accuracies_sgd = k_fold_cross_validation(\n",
        "    X_train, y_train_one_hot, layer_sizes, k=5, epochs=100, batch_size=batch_size, learning_rate=learning_rate, l2_lambda=l2_lambda, optimizer='sgd', augment=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1uiHWBGxIf1"
      },
      "outputs": [],
      "source": [
        "print(\"\\nK-Fold Cross-Validation with Adam (Augmented):\")\n",
        "avg_train_losses_adam, avg_val_losses_adam, avg_train_accuracies_adam, avg_val_accuracies_adam = k_fold_cross_validation(\n",
        "    X_train, y_train_one_hot, layer_sizes, k=5, epochs=100, batch_size=batch_size, learning_rate=learning_rate, l2_lambda=l2_lambda, optimizer='adam', augment=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmcor1CdxJc6"
      },
      "outputs": [],
      "source": [
        "# Ploting k-fold Cross-Validation Results (with SGD & Adam)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.plot(avg_train_losses_sgd, label='Train Loss SGD')\n",
        "ax1.plot(avg_val_losses_sgd, label='Val Loss SGD')\n",
        "ax1.plot(avg_train_losses_adam, label='Train Loss Adam')\n",
        "ax1.plot(avg_val_losses_adam, label='Val Loss Adam')\n",
        "ax1.set_title('Average Loss (K-Fold)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax2.plot(avg_train_accuracies_sgd, label='Train Acc SGD')\n",
        "ax2.plot(avg_val_accuracies_sgd, label='Val Acc SGD')\n",
        "ax2.plot(avg_train_accuracies_adam, label='Train Acc Adam')\n",
        "ax2.plot(avg_val_accuracies_adam, label='Val Acc Adam')\n",
        "ax2.set_title('Average Accuracy (K-Fold)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('kfold_loss_accuracy_sgd_adam.png')\n",
        "files.download('kfold_loss_accuracy_sgd_adam.png')\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5up1JcVrWVPY"
      },
      "outputs": [],
      "source": [
        "# =================== Load saved random_search_results.json file ================\n",
        "with open(\"random_search_results.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    best_params = data[\"best_params\"]\n",
        "    layer_sizes = data[\"layer_sizes\"]\n",
        "    best_val_accuracy = data[\"best_val_accuracy\"]\n",
        "\n",
        "learning_rate = best_params['learning_rate']\n",
        "l2_lambda = best_params['l2_lambda']\n",
        "batch_size = best_params['batch_size']\n",
        "optimizer = best_params['optimizer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rf3QaJZp_yE"
      },
      "source": [
        "## **10.Train final model with Best parameters on full train+val set:**\n",
        "\n",
        "\n",
        "*   Train models on combined train+val set, evaluate on test set, with and without augmentation.\n",
        "*   Note: We combine `X_train` and `X_val` into `X_train_val` to maximize training data (~ 18,532 samples) for the final model,after hyperparameter tuning. A new 10% validation set is split from `X_train_val` for early stopping to avoid using `X_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1NKAYqdqOGR"
      },
      "outputs": [],
      "source": [
        "# print(f\"\"\"## **10.Train final model with Best parameters on full train+val set:**\n",
        "\n",
        "\n",
        "# *   Train models on combined train+val set, evaluate on test set, with and without augmentation.\n",
        "# *   Note: We combine `X_train` and `X_val` into `X_train_val` to maximize training data (~ {X_train_final.shape[0]} samples) for the final model,after hyperparameter tuning. A new 10% validation set is split from `X_train_val` for early stopping to avoid using `X_test`.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27j2tVf5HVq7"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_val = np.concatenate([X_train, X_val])\n",
        "y_train_val_one_hot = np.concatenate([y_train_one_hot, y_val_one_hot])\n",
        "\n",
        "# Split a new validation set from X_train_val\n",
        "X_train_final, X_val_final, y_train_final_one_hot, y_val_final_one_hot = train_test_split(\n",
        "    X_train_val, y_train_val_one_hot, test_size=0.1, stratify=np.argmax(y_train_val_one_hot, axis=1), random_state=42\n",
        ")\n",
        "print(f\"Final Train: {X_train_final.shape[0]}, Final Val: {X_val_final.shape[0]}, Test: {X_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2QEkaaDWbIV"
      },
      "source": [
        "### **Case 1:  No Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iubOnS90WC0E"
      },
      "outputs": [],
      "source": [
        "# Case 1: No Augmentation\n",
        "nn = MulticlassNeuralNetwork(layer_sizes, learning_rate=best_params['learning_rate'],\n",
        "                             l2_lambda=best_params['l2_lambda'], optimizer=best_params['optimizer'])\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
        "    X_train_final, y_train_final_one_hot, X_val_final, y_val_final_one_hot, epochs=100,\n",
        "    batch_size=best_params['batch_size'], augment=False, early_stopping=True, verbose=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "112B9NP4NN5W"
      },
      "outputs": [],
      "source": [
        "y_pred = nn.predict(X_test)\n",
        "print(\"\\nTest Set Classification Report (No Augmentation):\")\n",
        "print(classification_report(np.argmax(y_test_one_hot, axis=1), y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "test_accuracy = np.mean(y_pred == np.argmax(y_test_one_hot, axis=1))\n",
        "print(f\"Test Accuracy (No Augmentation): {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJAKpoVqW-T7"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(np.argmax(y_test_one_hot, axis=1), y_pred)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix (No Augmentation)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.savefig('confusion_matrix_no_aug.png')\n",
        "files.download('confusion_matrix_no_aug.png')\n",
        "# plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0LetlG5XAGu"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(val_losses, label='Validation Loss')\n",
        "ax1.set_title('Loss Curve (No Augmentation)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax2.plot(train_accuracies, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
        "ax2.set_title('Accuracy Curve (No Augmentation)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_accuracy_no_aug.png')\n",
        "files.download('loss_accuracy_no_aug.png')\n",
        "# plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVVNVU97WhpX"
      },
      "source": [
        "### **Case 2:  With Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn5Eol6rWGkv"
      },
      "outputs": [],
      "source": [
        "# Case 2: With Augmentation\n",
        "nn_aug = MulticlassNeuralNetwork(layer_sizes, learning_rate=best_params['learning_rate'],\n",
        "                                 l2_lambda=best_params['l2_lambda'], optimizer=best_params['optimizer'])\n",
        "train_losses_aug, val_losses_aug, train_accuracies_aug, val_accuracies_aug = nn_aug.train(\n",
        "    X_train_final, y_train_final_one_hot, X_val_final, y_val_final_one_hot, epochs=100,\n",
        "    batch_size=best_params['batch_size'], augment=True, early_stopping=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3Kh3Vy4NVHI"
      },
      "outputs": [],
      "source": [
        "y_pred_aug = nn_aug.predict(X_test)\n",
        "print(\"\\nTest Set Classification Report (With Augmentation):\")\n",
        "print(classification_report(np.argmax(y_test_one_hot, axis=1), y_pred_aug, target_names=label_encoder.classes_))\n",
        "\n",
        "test_accuracy_aug = np.mean(y_pred_aug == np.argmax(y_test_one_hot, axis=1))\n",
        "print(f\"Test Accuracy (With Augmentation): {test_accuracy_aug:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB0XDL0JW4Ul"
      },
      "outputs": [],
      "source": [
        "cm_aug = confusion_matrix(np.argmax(y_test_one_hot, axis=1), y_pred_aug)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_aug, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix (With Augmentation)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.savefig('confusion_matrix_with_aug.png')\n",
        "files.download('confusion_matrix_with_aug.png')\n",
        "# plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI0UXru7W6KS"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.plot(train_losses_aug, label='Train Loss')\n",
        "ax1.plot(val_losses_aug, label='Validation Loss')\n",
        "ax1.set_title('Loss Curve (With Augmentation)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax2.plot(train_accuracies_aug, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies_aug, label='Validation Accuracy')\n",
        "ax2.set_title('Accuracy Curve (With Augmentation)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_accuracy_with_aug.png')\n",
        "files.download('loss_accuracy_with_aug.png')\n",
        "# plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5e-ipofzKYJ"
      },
      "source": [
        "## **11.Alternative: Train on X_train only, use X_val for early stopping**\n",
        "This section trains the model on `X_train` (~ 16,908 samples) with `X_val` (~ 5,637 samples) for early stopping to compare against the main approach (`X_train_val` with a 10% validation split)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i17UCaJ90YVe"
      },
      "source": [
        "### **Case 1:  No Augmentation (Alternative)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uLc-4obdKnJ5"
      },
      "outputs": [],
      "source": [
        "# Case 1: No Augmentation (Alternative)\n",
        "nn_alt = MulticlassNeuralNetwork(layer_sizes, learning_rate=best_params['learning_rate'],\n",
        "                                 l2_lambda=best_params['l2_lambda'], optimizer=best_params['optimizer'])\n",
        "train_losses_alt, val_losses_alt, train_accuracies_alt, val_accuracies_alt = nn_alt.train(\n",
        "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100,\n",
        "    batch_size=best_params['batch_size'], augment=False, early_stopping=True, verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyVJODVaOIcs"
      },
      "outputs": [],
      "source": [
        "y_pred_alt = nn_alt.predict(X_test)\n",
        "print(\"\\nTest Set Classification Report (No Augmentation, Alternative):\")\n",
        "print(classification_report(np.argmax(y_test_one_hot, axis=1), y_pred_alt, target_names=label_encoder.classes_))\n",
        "\n",
        "test_accuracy_alt = np.mean(y_pred_alt == np.argmax(y_test_one_hot, axis=1))\n",
        "print(f\"Test Accuracy (No Augmentation, Alternative): {test_accuracy_alt:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWcQBJdi0AZy"
      },
      "outputs": [],
      "source": [
        "cm_alt = confusion_matrix(np.argmax(y_test_one_hot, axis=1), y_pred_alt)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_alt, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix (No Augmentation, Alternative)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.savefig('confusion_matrix_no_aug_alt.png')\n",
        "files.download('confusion_matrix_no_aug_alt.png')\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc0ytqlJ0AND"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.plot(train_losses_alt, label='Train Loss')\n",
        "ax1.plot(val_losses_alt, label='Validation Loss')\n",
        "ax1.set_title('Loss Curve (No Augmentation, Alternative)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax2.plot(train_accuracies_alt, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies_alt, label='Validation Accuracy')\n",
        "ax2.set_title('Accuracy Curve (No Augmentation, Alternative)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_accuracy_no_aug_alt.png')\n",
        "files.download('loss_accuracy_no_aug_alt.png')\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw6j11lq0msf"
      },
      "source": [
        "### **Case 2:  With Augmentation (Alternative)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT3ajUNo0L3x"
      },
      "outputs": [],
      "source": [
        "# Case 2: With Augmentation (Alternative)\n",
        "nn_aug_alt = MulticlassNeuralNetwork(layer_sizes, learning_rate=best_params['learning_rate'],\n",
        "                                     l2_lambda=best_params['l2_lambda'], optimizer=best_params['optimizer'])\n",
        "train_losses_aug_alt, val_losses_aug_alt, train_accuracies_aug_alt, val_accuracies_aug_alt = nn_aug_alt.train(\n",
        "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100,\n",
        "    batch_size=best_params['batch_size'], augment=True, early_stopping=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY6G2S-p0L0Y"
      },
      "outputs": [],
      "source": [
        "y_pred_aug_alt = nn_aug_alt.predict(X_test)\n",
        "print(\"\\nTest Set Classification Report (With Augmentation, Alternative):\")\n",
        "print(classification_report(np.argmax(y_test_one_hot, axis=1), y_pred_aug_alt, target_names=label_encoder.classes_))\n",
        "\n",
        "test_accuracy_aug_alt = np.mean(y_pred_aug_alt == np.argmax(y_test_one_hot, axis=1))\n",
        "print(f\"Test Accuracy (With Augmentation, Alternative): {test_accuracy_aug_alt:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBn9550A06zH"
      },
      "outputs": [],
      "source": [
        "cm_aug_alt = confusion_matrix(np.argmax(y_test_one_hot, axis=1), y_pred_aug_alt)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_aug_alt, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix (With Augmentation, Alternative)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.savefig('confusion_matrix_with_aug_alt.png')\n",
        "files.download('confusion_matrix_with_aug_alt.png')\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpC5jkmlOZzt"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.plot(train_losses_aug_alt, label='Train Loss')\n",
        "ax1.plot(val_losses_aug_alt, label='Validation Loss')\n",
        "ax1.set_title('Loss Curve (With Augmentation, Alternative)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax2.plot(train_accuracies_aug_alt, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies_aug_alt, label='Validation Accuracy')\n",
        "ax2.set_title('Accuracy Curve (With Augmentation, Alternative)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_accuracy_with_aug_alt.png')\n",
        "files.download('loss_accuracy_with_aug_alt.png')\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtA257mt2CgX"
      },
      "outputs": [],
      "source": [
        "# Comparison Bar Chart\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "approaches = ['Main (No Aug)', 'Main (With Aug)', 'Alt (No Aug)', 'Alt (With Aug)']\n",
        "accuracies = [test_accuracy, test_accuracy_aug, test_accuracy_alt, test_accuracy_aug_alt]\n",
        "ax.bar(approaches, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "ax.set_title('Test Accuracy Comparison: Main vs. Alternative')\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_ylim(0, 1)\n",
        "for i, v in enumerate(accuracies):\n",
        "    ax.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_accuracy_comparison.png')\n",
        "files.download('test_accuracy_comparison.png')\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yq6HiCe15DT"
      },
      "outputs": [],
      "source": [
        "comparison_results = {\n",
        "    'main_no_aug': test_accuracy,\n",
        "    'main_with_aug': test_accuracy_aug,\n",
        "    'alt_no_aug': test_accuracy_alt,\n",
        "    'alt_with_aug': test_accuracy_aug_alt\n",
        "}\n",
        "with open('comparison_results.json', 'w') as f:\n",
        "    json.dump(comparison_results, f)\n",
        "files.download('comparison_results.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ybvmW1wJUHC"
      },
      "source": [
        "## **12. Results Summary**\n",
        "- Best parameters: {'learning_rate': 0.001, 'l2_lambda': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
        "- Best validation accuracy: 82.93%\n",
        "- **Main Approach**:\n",
        "  - Test accuracy (No Augmentation): 91.64%\n",
        "  - Test accuracy (With Augmentation): 84.55%\n",
        "- **Alternative Approach**:\n",
        "  - Test accuracy (No Augmentation): 90.69%\n",
        "  - Test accuracy (With Augmentation): 83.95%\n",
        "- Figure: Test accuracy comparison (`test_accuracy_comparison.png`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwLFSG1r4zOs"
      },
      "outputs": [],
      "source": [
        "# Generate Markdown:\n",
        "print(f\"\"\"## **12. Results Summary**\n",
        "- Best parameters: {best_params}\n",
        "- Best validation accuracy: {best_val_accuracy:.2%}\n",
        "- **Main Approach**:\n",
        "  - Test accuracy (No Augmentation): {test_accuracy:.2%}\n",
        "  - Test accuracy (With Augmentation): {test_accuracy_aug:.2%}\n",
        "- **Alternative Approach**:\n",
        "  - Test accuracy (No Augmentation): {test_accuracy_alt:.2%}\n",
        "  - Test accuracy (With Augmentation): {test_accuracy_aug_alt:.2%}\n",
        "- Figure: Test accuracy comparison (`test_accuracy_comparison.png`)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QDpgxDo4wkO"
      },
      "outputs": [],
      "source": [
        "# with open(\"results_summary.md\", \"w\") as f:\n",
        "#     f.write(\"## 12. Results Summary\\n\")\n",
        "#     f.write(f\"- Best parameters: {best_params}\\n\")\n",
        "#     f.write(f\"- Best validation accuracy: {best_val_accuracy:.2%}\\n\")\n",
        "#     f.write(\"- **Main Approach**:\\n\")\n",
        "#     f.write(f\"  - Test accuracy (No Augmentation): {test_accuracy:.2%}\\n\")\n",
        "#     f.write(f\"  - Test accuracy (With Augmentation): {test_accuracy_aug:.2%}\\n\")\n",
        "#     f.write(\"- **Alternative Approach**:\\n\")\n",
        "#     f.write(f\"  - Test accuracy (No Augmentation): {test_accuracy_alt:.2%}\\n\")\n",
        "#     f.write(f\"  - Test accuracy (With Augmentation): {test_accuracy_aug_alt:.2%}\\n\")\n",
        "#     f.write(\"- Figure: Test accuracy comparison (`test_accuracy_comparison.png`)\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOUSbEo-QrHy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}